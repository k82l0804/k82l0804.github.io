---
layout: default
title: RAG Concepts
permalink: /ai-lab/rag/
---

# RAG Concepts

Understanding Retrieval-Augmented Generation, MCP, and embeddings.

---

## Documentation

| Document | Description |
|----------|-------------|
| [ðŸ“š Understanding RAG & MCP](concepts) | Complete beginner's guide to RAG and MCP servers |

---

## What is RAG?

**RAG (Retrieval-Augmented Generation)** enhances LLM responses by:

1. **Retrieving** relevant context from a knowledge base
2. **Augmenting** the LLM prompt with that context  
3. **Generating** better, more informed responses

> Think of it as giving the LLM a "cheat sheet" of relevant code before answering.

---

## The RAG Pipeline

```
Phase 1: Indexing (Offline)
  Your Codebase â†’ Chunking â†’ Embedding â†’ Vector Storage

Phase 2: Query (Every Question)
  Your Question â†’ Embed Query â†’ Search Vectors â†’ Retrieve Chunks â†’ Augment Prompt â†’ Generate Answer
```

---

## Can You Keep Data Private? YES! âœ…

The entire pipeline can run locally:
- âœ… Local embeddings model (sentence-transformers, llama.cpp)
- âœ… Local vector database (LanceDB)
- âœ… Local MCP server
- âœ… Local LLM (vLLM, llama-server)
- **Nothing leaves your machine!**

[Full privacy architecture â†’](concepts#can-you-keep-private-data-private-yes-)

---

*Private, context-aware coding assistance.*
